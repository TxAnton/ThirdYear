---
title: "HW_3/9"
author: "Larin Anton"
date: "12/18/2020"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
x <- c(1,3,0,3,1,2,1,2,1,2,2,3,1,2,1,3,2,1,1,1,2,3,2,1,3,1,1,1,3,1,0,1,1,3,1,3,2,1,2,1,2,2,1,0,2,1,2,1,1,2)
y <- c(9.60, 6.02, 3.23, 8.41, 3.63, 8.25, 6.25, 7.48, 7.61, 5.64, 4.40, 4.09, 7.04, 8.93, 6.02, 8.49, 7.08,
         7.58, 8.20, 7.47, 7.47, 6.60, 7.14, 5.18, 3.54, 5.28, 5.77, 4.90, 4.67, 5.70, 6.88, 4.53, 7.26, 8.18,
         5.22, 11.85, 8.58, 2.82, 2.43, 3.65, 6.67, 8.08, 6.76, 4.47, 4.04, 8.61, 9.39, 8.38, 4.79, 9.56)

al<- 0.2
h <-1.4
```

## ИДЗ 3 Статан
## ИДЗ 9 Матпакеты


![Image alt](https://github.com/TxAnton/ThirdYear/raw/master/MS/IDZ3/Statan_3_12.png)


#Ход работы
##Графические представление

```{r}
plot(x,y,main="Result")
```

Линейная регрессионная модель:
\(y = \beta_0 + \beta_1*x\)

```{r, include=FALSE}

```

```{r, include=FALSE}
dat<-data.frame(y=y,x=x)
dat1<-dat[order(x),]
```


## 1  
МНК оценка параметров сдвига \(\beta_0\) и масштаба \(\beta_1\)

```{r,eval}
n<-length(y)
x0<-array(1,dim=n)
X<-t(matrix(c(x0,x),nrow=n,ncol=2))
Y<-as.matrix(y)
S<-X%*%t(X)
S1<-solve(S)
bhat<-S1%*%X%*%Y
```
#### Результат:  
\(\beta_0 = `r bhat[1]`\)   
\(\beta_1 = `r bhat[2]`\)   

Нарисуем полученную регрессионную зависимость

```{r,echo=FALSE}
plot(x, y)
x1 <- seq(from = min(x), to = max(x), by = (0.5))
y1 <- bhat[1] + bhat[2] * x1
points(x1, y1, "l", col="red", lwd=2)
```


## 2
Построение несмещенной оценки дисперсии
```{r}
  res<-Y-t(X)%*%as.matrix(bhat)
SS<-sum(res^2)
s2<-SS/(n-2)
```
Результат: \(`r s2`\)

Построение гистограммы с шагом h = `r h`  на базе ошибок

```{r, echo=FALSE}
brk<-seq(min(res), max(res) + h, by=h)
hist(res,breaks=brk)
```

Проверка гипотезы нормальности ошибок на уровне \(\alpha\) по \(\chi^2\)
\(H_0: Y-X^T \beta\sim(0,\sigma^2)\)

```{r}
hh<-hist(res,breaks=brk,plot=FALSE)
nu<-hh$counts
breaks = hh$breaks;
r = length(breaks) - 1
l.b<-length(brk)
csq0<-function(s){
  if (s>0){
    p<-pnorm(brk[2:l.b],0,s)-pnorm(brk[1:(l.b-1)],0,s)
    return(sum((nu-n*p)^2/n/p))
  } else {
    return(Inf)
  }
}
csq.s<-nlm(csq0,p=sqrt(s2))$minimum
pv<-pchisq(csq.s,r - 2,lower.tail=FALSE)
```
Результат: `r csq.s<pv`

Оценим расстояние оценки до класса нормальных распределений оп Колмогорову
```{r}
kolm.stat<-function(s){
  sres<-sort(res)
  fdistr<-pnorm(sres,0,s)
  max(abs(c(0:(n-1))/n-fdistr),abs(c(1:n)/n-fdistr))
}
ks.dist<-nlm(kolm.stat,p=sqrt(s2))$minimum


```
Полученое расстояние: `r ks.dist`

```{r,echo=FALSE}
x2<-c(0:1000)*(max(res)-min(res))/1000+min(res)
y2<-pnorm(x2,0,nlm(kolm.stat,p=sqrt(s2))$estimate)

plot.ecdf(res)
points(x2,y2,"l",col="red",lwd=2)
```

## 3

Построим ДИ для параметров с уровнем доверия \(1-\alpha (`r 1-al`)\)

```{r}
C<-diag(c(1,1))
ph<-bhat
V<-diag(S1)
xa<-qt(1-al/2,n-2)
s1<-sqrt(s2)
d<-xa*s1*sqrt(V)
CI<-data.frame(lw=ph-d,up=ph+d)
```
Для \(\beta_0: [`r CI[1,1]`,`r CI[1,2]`]\)  
Для \(\beta_1: [`r CI[2,1]`,`r CI[2,2]`]\)  

Доверительный эллипс можно вычислить как

\(A\alpha=\{x,y:((xy)^T-\beta)^T*(XX^T)^{-1}*((xy)^T-\beta)\le qs^2x\alpha\}\)  
Где:  

\(\beta=\)  
```{r,echo=FALSE}
print(bhat)
```
\((XX^T)=\)
```{r,echo=FALSE}
print(S1)
```

## 4
Гипотеза независимости Y от X:
\(H0: \beta1=0\)  
Критерий:  
\(\Phi(x)=\begin{cases} 0, \alpha<PV \\ 1,\alpha>PV \end{cases} \)

Найдем статистику F-критерия и P-значение:
```{r}
FST<-bhat[2]^2/V[2]/s2
pv.f<-pf(FST,1,n-2,lower.tail=FALSE)
```
Получим `r FST` и `r pv.f`  
Результат: `r al<pv.f`  

## 5 
Добавим в модель член с \(X^2\):  
\(Y=\beta_1+\beta_2x+\beta_3x^2+\varepsilon\)

Найдем МНК оценки


```{r}
x0 <- array(1, dim=n)
X <- t(matrix(c(x0, x, x^2), nrow=n, ncol=3))
Y <- as.matrix(y)
S <- X%*%t(X)
S1 <- solve(S)
bht <- S1%*%X%*%Y
```
\(\beta1=`r bht[1]`\)  
\(\beta2=`r bht[2]`\)  
\(\beta3=`r bht[3]`\)  

Нарисуем полученную регрессионную зависимость

```{r,echo=FALSE}
plot(x, y)
x1 <- seq(from = min(x), to = max(x), by = (0.5))
y1 <- bht[1] + bht[2] * x1 + bht[3] * x1^2
points(x1, y1, "l", col="red", lwd=2)
```

## 6
Несмещенная оценка дисперсии
```{r}
res<-Y-t(X)%*%as.matrix(bht)
SS<-sum(res^2)
s2<-SS/(n-2)
```
Результат: `r s2`

Гистограмма на базе ошибок
```{r}
brk<-seq(min(res), max(res) + h, by=h)
hist(res,breaks=brk)
```

Проверка гипотезы нормальности ошибок на уровне \(\alpha\) по \(\chi^2\)
```{r}
l.b<-length(brk)
brk[1]<- -Inf
brk[l.b]<-Inf
#r = length(breaks) - 1
hh<-hist(res,breaks=brk,plot=FALSE)
nu<-hh$counts
breaks = hh$breaks;
r = length(breaks) - 1

l.b<-length(brk)
csq0<-function(s){
  if (s>0){
    p<-pnorm(brk[2:l.b],0,s)-pnorm(brk[1:(l.b-1)],0,s)
    return(sum((nu-n*p)^2/n/p))
  } else {
    return(Inf)
  }
}
csq.s<-nlm(csq0,p=sqrt(s2))$minimum
pv<-pchisq(csq.s,r-3,lower.tail=FALSE)

```
Результат: `r csq.s<pv`  
  
Оценка расстояния до нормального рапределения по Колмагорову

```{r}
kolm.stat<-function(s){
  sres<-sort(res)
  fdistr<-pnorm(sres,0,s)
  max(abs(c(0:(n-1))/n-fdistr),abs(c(1:n)/n-fdistr))
}
ks.dist<-nlm(kolm.stat,p=sqrt(s2))$minimum


x2<-c(0:1000)*(max(res)-min(res))/1000+min(res)
y2<-pnorm(x2,0,nlm(kolm.stat,p=sqrt(s2))$estimate)
```

```{r,echo=FALSE}
plot.ecdf(res)
points(x2,y2,"l",col="red",lwd=2)
```

Полученое расстояние: `r ks.dist`

## 7

Построим ДИ для параметров

```{r}
C<-diag(c(1,1,1))
ph<-bht #t(C)%*%bhat
V<-diag(S1) # diag(C%*%S1%*%t(C))
xa<-qt(1-al/2,n-2)
s1<-sqrt(s2)
d<-xa*s1*sqrt(V)
CI<-data.frame(lw=ph-d,up=ph+d)
```
Полученые интервалы:
```{r,echo=FALSE}
print(CI)
```

Доверительный эллипсоид имеет форму

\(A\alpha=\{x,y:((xyz)^T-\beta)^T*(XX^T)^{-1}*((xyz)^T-\beta)\le 1\}\)  
Где:  
\(\beta=\)  
```{r,echo=FALSE}
print(bht)
```
\((XX^T)=\)
```{r,echo=FALSE}
print(S1)
```

## 8
Гипотеза линейной регрессионной зависимости Y от X:
\(H0: \beta3=0\)  
Критерий:  
\(\Phi(x)=\begin{cases} 0, \alpha<PV \\ 1,\alpha>PV \end{cases} \)
  
Найдем статистику F-критерия и P-значение:
```{r}
FST<-bht[3]^2/S1[2,2]/s2
pv.f<-pf(FST,1,n-2,lower.tail=FALSE)
```
Получим `r FST` и `r pv.f`  
Результат: `r al<pv.f` 

